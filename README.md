# Is this image real or AI-generated?

This project is a small model that tries to answer that question: **given an image, it tells you whether it looks more like a real photo or like something generated by AI** (e.g. DALL·E, Midjourney, Stable Diffusion). It’s useful for content moderation, fact-checking, or just satisfying your curiosity.

Below we explain how it works in plain terms, so you can understand the idea without needing to be a deep learning expert.

---

## Context: competition and how we built the data

This model **won an internal competition** at the **Facultad de Ciencias Exactas y Naturales**. To train and evaluate it, we needed a clear split between real and AI-generated images. We didn’t rely only on images from the internet: we **trained StyleGAN2** ourselves and used it to generate synthetic (AI) images. **Training and testing were done with those StyleGAN2-generated photos and a selfie dataset** (real human selfies). That way we had full control over the source and balance of both classes.

We are **providing the validation dataset** so that others can reproduce or compare results. Below are the metrics we obtained on that validation set.

---

## Validation metrics

**Classification results by class (threshold = 0)**

- True negative rate: 94 / 98 (95.92%)
- True positive rate: 99 / 99 (100.00%)

**F1 score**: **0.9792**

**AUC-ROC: 1.0000**

---

## The big idea

We don’t build the “vision” part from zero. We take a model that already knows a lot about images — **CLIP**, from OpenAI — and we only train a tiny extra part on top that learns to say “this looks real” or “this looks AI-generated.” So the architecture is:

```
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│  Input images   │ ──► │ CLIP Vision      │ ──► │ Pooler output   │
│  (any size)     │     │ (ViT-L/14)       │     │ (1024-dim)      │
└─────────────────┘     └──────────────────┘     └────────┬────────┘
                                                          │
                                                          ▼
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│  Output score   │ ◄── │ Linear + sigmoid │ ◄── │ Dropout (0.35)   │
│  in [-1, 1]     │     │ (1024 → 1)       │     │                 │
└─────────────────┘     └──────────────────┘     └─────────────────┘
```

---

## Step by step: what happens when you feed an image

### 1. The image gets preprocessed

Before anything else, the image is resized and normalized the same way CLIP expects. So whether you pass a huge photo or a small thumbnail, the model always receives a consistent format. You don’t have to worry about that — the code uses CLIP’s own preprocessing.

### 2. CLIP “encodes” the image

CLIP is a vision model that was trained on a huge amount of image–text pairs. Here we only use its **vision part**: it takes the image and turns it into a single vector of 1024 numbers. That vector is a compact representation of what’s in the image — shapes, textures, colors, etc.

We keep this part **frozen** (we don’t train it). So we’re not changing how CLIP “sees”; we only use what it already knows. That makes training faster and more stable.

### 3. Our little head makes the decision

That 1024-dimensional vector is then passed to our own small “head”:

- First, **dropout** is applied (we randomly zero out some values during training). This helps the model not memorize the training set and generalize better.
- Then a **single linear layer** (1024 numbers in → 1 number out) turns the vector into one raw score.
- Finally, we squash that score with a **sigmoid** and scale it so the output lives between **-1 and 1**. That’s the number you get: the closer to one extreme or the other, the more confident the model is.

So the “brain” we actually train is just: dropout + one linear layer. All the visual understanding comes from CLIP.

### 4. What the score means

The model outputs a value between **-1** and **1**. Depending on how the model was trained, one end will mean “more likely real” and the other “more likely AI-generated.” You can set a threshold (e.g. 0) and say: below = AI, above = real. The further from zero, the more confident the prediction.

---

## What you need to run it

- **PyTorch** and the **transformers** library (Hugging Face), so we can load CLIP and run the model.
- The file **`modelo_final.pth`** in the right place (by default the code looks for it in the current directory). That file contains the trained weights of our little head (and any state that was saved); CLIP’s weights are downloaded automatically when you first use it.

---

## How to use it in code

You create the model, put it in evaluation mode, and pass it your images. The model will preprocess them with CLIP and return a score for each one:

```python
from modelo.modelo import grpClasificador
import torch

# Load the model (it will load the weights from modelo_final.pth)
model = grpClasificador()
model.eval()

# Pass your images (tensor or list of PIL images)
with torch.no_grad():
    scores = model(images)

# scores is between -1 and 1 for each image
# Interpret based on how you trained it (e.g. < 0 → AI, > 0 → real)
```

You can also tweak things like which CLIP model to use, whether to freeze it, or the dropout rate, by passing arguments into `grpClasificador` — check the class in `modelo/modelo.py` for the full list.

---



